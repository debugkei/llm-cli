# llm-cli
This project allows CLI use of LLMs through any server that supports OpenAI API format of hosting LLMs.  
I used the program with llama.cpp's server.  

# Features
* Get responses from terminal.
* Input files and whole directories into LLMs for them to read.
* Execute commands generated by LLMs right in the terminal.
* Requires absolutely no GUI to install and use.  

# Installation
## Prerequisites
* [git](https://git-scm.com/)
* [python](https://www.python.org/)
* pip, comes with python
* (Optional) python-venv, comes with python

## Without venv:
### Cross platform way:
```sh
git clone https://github.com/debugkei/llm-cli
cd llm-cli
pip install -r requirements.txt
```
## With venv:
### Linux:
```sh
git clone `https://github.com/debugkei/llm-cli`  
cd llm-cli  
python -m venv  
source venv/bin/activate  
pip install -r requirements.txt  
```
### Windows:
```sh
git clone `https://github.com/debugkei/llm-cli`  
cd llm-cli  
python -m venv  
venv\Scripts\activate  
pip install -r requirements.txt  
```

# Usage
```sh
python src/llm-cli.py your_URL your_model_name your_prompt  
```
`-f` - file for LLM to consider.  
`-t` - set the temperature for the LLM.  
`-d` - directory for LLM to consider.  
`-a` - API, specify only if not running locally.  
`-h` - print out possible flags with their description.  

If that's too long to type I recommend creating [Alias](##alias-creation).  
Alias will bind the URL, model and optionally API as arguments for llm-cli, and will allow use from anywhere.  
## Alias creation:
#### Linux bash:
```bash
echo 'alias llm-cli='python path/to/llm-cli.py your_URL your_model_name -a your_API_(optional)'' >> ~/.bashrc  
```

#### Linux zsh:
```zsh
echo 'alias llm-cli='python path/to/llm-cli.py your_URL your_model_name -a your_API_(optional)'' >> ~/.zshrc  
```

#### Windows powershell:
```powershell
Add-Content -Path $PROFILE -Value "function llm-cli { python path/to/llm-cli.py your_URL your_model_name -a your_API_(optional) }"  
```

## Usage example with llama.cpp
1. Download precompiled binary from [here](https://github.com/ggml-org/llama.cpp/releases) for your platform and GPU rendering API.  
  Or to download via terminal:
  ```sh
  git clone https://github.com/ggml-org/llama.cpp
  ```
  And then build it, how to build is [here](https://github.com/ggml-org/llama.cpp#building-the-project)  
2. Download a model from [here](https://huggingface.co/)  
3. Start the server:  
```sh
cd to/your/llama.cpp/installation  
./llama-server -m /path/to/your/model.gguf -c 2048 -ngl 200  
```
`-ngl` - amount of work offloaded to gpu, everything above 100 is 100, and if you planning on using CPU, then just dont include -ngl.  
`-c` - context, for bigger prompts, files or directories use greater values.  
4. [Usage](#usage)  
  * URL is `http://localhost:8080`
  * Model name is model file name without .gguf

# Deletion
1. Delete the directory with llm-cli.  
2. Delete the alias, if created. Just erase it from .bashrc, or .zshrc, or from $PROFILE.

# Dependencies
## To install all
```sh
pip install -r requirements.txt  
```
## List
* openai (python package)

# Development
### This project is actively developed right now.
* No pull requests will be accepted.
